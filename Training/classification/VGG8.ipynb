{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11448574,"sourceType":"datasetVersion","datasetId":7172745},{"sourceId":11967454,"sourceType":"datasetVersion","datasetId":7525284}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\n# Reproducibility\nmanualSeed = 2019\nrandom.seed(manualSeed)\nnp.random.seed(manualSeed)\ntorch.manual_seed(manualSeed)\ntorch.cuda.manual_seed_all(manualSeed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    print(\"worker_seed\", worker_seed)\n\ng = torch.Generator().manual_seed(manualSeed)\n\n# Define transform (no augmentation)\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# Load dataset with transform\ntrain_dir = \"/kaggle/input/mapping-isic2019-training-dataset/Training_By_Class\"\ndataset = datasets.ImageFolder(root=train_dir, transform=transform)\nprint(\"load dataset done\", len(dataset))\n\nclass_names = dataset.classes\nprint(\"class_names\", class_names)\nclass_to_idx = dataset.class_to_idx\nprint(\"Danh sách class và chỉ số tương ứng:\", class_to_idx)\n\n# K-fold cross-validation (5 folds)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Check available GPUs\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n\nskf = StratifiedKFold(n_splits=5)\nlabels = [label for _, label in dataset]\nprint(\"K-fold cross-validation done\", len(labels), \"labels\")\n\n# Build model architecture\nclass CNN_VGG8(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3)\n        self.conv2 = nn.Conv2d(64, 128, 3)\n        self.conv3 = nn.Conv2d(128, 256, 3)\n        self.conv4 = nn.Conv2d(256, 512, 3)\n        self.conv5 = nn.Conv2d(512, 512, 3)\n        self.fc1 = nn.Linear(2048, 768)\n        self.fc2 = nn.Linear(768, 512)\n        self.fc3 = nn.Linear(512, 9)\n    \n    def forward(self, X):\n        out = F.max_pool2d(F.relu(self.conv1(X)), kernel_size=2, stride=2)\n        out = F.max_pool2d(F.relu(self.conv2(out)), kernel_size=2, stride=2)\n        out = F.max_pool2d(F.relu(self.conv3(out)), kernel_size=2, stride=2)\n        out = F.max_pool2d(F.relu(self.conv4(out)), kernel_size=2, stride=2)\n        out = F.max_pool2d(F.relu(self.conv5(out)), kernel_size=2, stride=2)\n        out = torch.flatten(out, 1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n        return out\n\n# Function to plot and save confusion matrix\ndef plot_confusion_matrix(cm, class_names, title, filename):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title(title)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.savefig(filename)\n    plt.close()\n\n# Trainer function with metrics, confusion matrix per epoch, and early stopping\ndef trainer(model, epochs, train_data, val_data, fold_idx):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=0.0005)\n    best_accuracy_val = 0\n    patience = 5  # Stop if no improvement for 5 epochs\n    no_improvement_count = 0\n    for epoch in range(1, epochs + 1):\n        # Training phase\n        model.train()\n        correct = 0\n        total = 0\n        total_loss = 0.0\n        train_label_list = []\n        train_predict_list = []\n        for i, data in enumerate(train_data):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            out = model(inputs)\n            _, predicted = torch.max(out, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n            loss = criterion(out, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            train_label_list.extend(labels.cpu().numpy())\n            train_predict_list.extend(predicted.cpu().numpy())\n        \n        accuracy_train = 100 * correct / total\n        train_report = classification_report(train_label_list, train_predict_list, output_dict=True, zero_division=0)\n        train_precision = train_report['weighted avg']['precision'] * 100\n        train_recall = train_report['weighted avg']['recall'] * 100\n        train_f1 = train_report['weighted avg']['f1-score'] * 100\n        \n        # Compute and save training confusion matrix\n        train_cm = confusion_matrix(train_label_list, train_predict_list)\n        plot_confusion_matrix(\n            train_cm, class_names,\n            title=f'Confusion Matrix (Train) - Fold {fold_idx + 1}, Epoch {epoch}',\n            filename=f'confusion_matrix_fold_{fold_idx + 1}_epoch_{epoch}_train.png'\n        )\n        \n        # Validation phase\n        model.eval()\n        correct = 0\n        total = 0\n        val_label_list = []\n        val_predict_list = []\n        val_probs_list = []\n        with torch.no_grad():\n            for i, data in enumerate(val_data):\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n                out = model(inputs)\n                probs = F.softmax(out, dim=1)\n                _, predicted = torch.max(out, 1)\n                val_label_list.extend(labels.cpu().numpy())\n                val_predict_list.extend(predicted.cpu().numpy())\n                val_probs_list.extend(probs.cpu().numpy())\n                correct += (predicted == labels).sum().item()\n                total += labels.size(0)\n        \n        accuracy_val = 100 * correct / total\n        val_report = classification_report(val_label_list, val_predict_list, output_dict=True, zero_division=0)\n        val_precision = val_report['weighted avg']['precision'] * 100\n        val_recall = val_report['weighted avg']['recall'] * 100\n        val_f1 = val_report['weighted avg']['f1-score'] * 100\n        \n        # Compute and save validation confusion matrix\n        val_cm = confusion_matrix(val_label_list, val_predict_list)\n        plot_confusion_matrix(\n            val_cm, class_names,\n            title=f'Confusion Matrix (Validation) - Fold {fold_idx + 1}, Epoch {epoch}',\n            filename=f'confusion_matrix_fold_{fold_idx + 1}_epoch_{epoch}_val.png'\n        )\n        \n        # Log metrics for the epoch\n        print(f\"Epoch {epoch}: \"\n              f\"Loss = {total_loss / len(train_data):.4f}, \"\n              f\"Train Accuracy = {accuracy_train:.2f}%, \"\n              f\"Train Precision = {train_precision:.2f}%, \"\n              f\"Train Recall = {train_recall:.2f}%, \"\n              f\"Train F1 = {train_f1:.2f}%, \"\n              f\"Val Accuracy = {accuracy_val:.2f}%, \"\n              f\"Val Precision = {val_precision:.2f}%, \"\n              f\"Val Recall = {val_recall:.2f}%, \"\n              f\"Val F1 = {val_f1:.2f}%\")\n        \n        # Early stopping logic\n        if accuracy_val > best_accuracy_val:\n            best_accuracy_val = accuracy_val\n            no_improvement_count = 0\n        else:\n            no_improvement_count += 1\n            if no_improvement_count >= patience:\n                print(f\"Early stopping at epoch {epoch}\")\n                break\n    \n    # Final validation metrics\n    final_report = classification_report(val_label_list, val_predict_list, output_dict=True, zero_division=0)\n    final_conf_matrix = confusion_matrix(val_label_list, val_predict_list)\n    \n    # Save final validation confusion matrix\n    plot_confusion_matrix(\n        final_conf_matrix, class_names,\n        title=f'Final Confusion Matrix (Validation) - Fold {fold_idx + 1}',\n        filename=f'final_confusion_matrix_fold_{fold_idx + 1}_val.png'\n    )\n    \n    # ROC Curve for each class\n    fpr = {}\n    tpr = {}\n    roc_auc = {}\n    for i in range(len(class_names)):\n        fpr[i], tpr[i], _ = roc_curve(np.array(val_label_list) == i, np.array(val_probs_list)[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    return (accuracy_train, best_accuracy_val, total_loss / len(train_data), final_report,\n            final_conf_matrix, fpr, tpr, roc_auc, val_label_list, val_predict_list)\n\n# Main loop\nmodel_list = [CNN_VGG8() for _ in range(5)]\nfold_metrics = defaultdict(list)\nbest_val = 0\nbest_fold = 0\n\nfor i_fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n    train_dataset = Subset(dataset, train_idx)\n    val_dataset = Subset(dataset, val_idx)\n    \n    # Optimized DataLoader with larger batch size\n    fold_train_dataloader = DataLoader(\n        train_dataset, \n        batch_size=256,  # 128 per GPU if using 2 GPUs\n        shuffle=True, \n        num_workers=4, \n        worker_init_fn=seed_worker, \n        generator=g\n    )\n    fold_val_dataloader = DataLoader(\n        val_dataset, \n        batch_size=256, \n        shuffle=False, \n        num_workers=4, \n        worker_init_fn=seed_worker, \n        generator=g\n    )\n    \n    # Initialize and wrap model with DataParallel\n    model = model_list[i_fold].to(device)\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)  # Distribute model across GPUs\n        \n    print(torch.cuda.memory_summary())\n    print(f\"\\nFold {i_fold + 1}: Training...\")\n    \n    # Train the model\n    (accuracy_train, accuracy_val, loss, report, conf_matrix, fpr, tpr, roc_auc,\n     val_label_list, val_predict_list) = trainer(\n        model=model,\n        epochs=30,\n        train_data=fold_train_dataloader,\n        val_data=fold_val_dataloader,\n        fold_idx=i_fold\n    )\n    \n    # Store metrics\n    fold_metrics['accuracy_train'].append(accuracy_train)\n    fold_metrics['accuracy_val'].append(accuracy_val)\n    fold_metrics['precision'].append(report['weighted avg']['precision'] * 100)\n    fold_metrics['recall'].append(report['weighted avg']['recall'] * 100)\n    fold_metrics['f1_score'].append(report['weighted avg']['f1-score'] * 100)\n    fold_metrics['confusion_matrix'].append(conf_matrix)\n    fold_metrics['roc_auc'].append(roc_auc)\n    \n    # Print fold metrics\n    print(f\"\\nFold {i_fold + 1} Metrics:\")\n    print(f\"Train Accuracy: {accuracy_train:.2f}%\")\n    print(f\"Val Accuracy: {accuracy_val:.2f}%\")\n    print(f\"Precision: {report['weighted avg']['precision'] * 100:.2f}%\")\n    print(f\"Recall: {report['weighted avg']['recall'] * 100:.2f}%\")\n    print(f\"F1 Score: {report['weighted avg']['f1-score'] * 100:.2f}%\")\n    print(\"Confusion Matrix:\\n\", conf_matrix)\n    print(\"Classification Report:\\n\", classification_report(val_label_list, val_predict_list, target_names=class_names, zero_division=0))\n    \n    # Plot ROC Curve for each class\n    plt.figure(figsize=(10, 8))\n    for i in range(len(class_names)):\n        plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'ROC Curve for Fold {i_fold + 1}')\n    plt.legend(loc=\"lower right\")\n    plt.savefig(f'roc_curve_fold_{i_fold + 1}.png')\n    plt.close()\n    \n    # Save the best model (unwrap DataParallel if necessary)\n    if accuracy_val > best_val:\n        best_val = accuracy_val\n        best_fold = i_fold\n        if isinstance(model, nn.DataParallel):\n            torch.save(model.module.state_dict(), '/kaggle/working/best_cancer_prediction.pth')\n        else:\n            torch.save(model.state_dict(), '/kaggle/working/best_cancer_prediction.pth')\n\n# Final summary of folds\nprint(\"\\nFinal Summary of All Folds:\")\nfor i in range(5):\n    print(f\"\\nFold {i + 1}:\")\n    print(f\"Train Accuracy: {fold_metrics['accuracy_train'][i]:.2f}%\")\n    print(f\"Val Accuracy: {fold_metrics['accuracy_val'][i]:.2f}%\")\n    print(f\"Precision: {fold_metrics['precision'][i]:.2f}%\")\n    print(f\"Recall: {fold_metrics['recall'][i]:.2f}%\")\n    print(f\"F1 Score: {fold_metrics['f1_score'][i]:.2f}%\")\n\nprint(f\"\\nThe best model is from fold {best_fold + 1} with validation accuracy {best_val:.2f}%\")\n\n# Evaluate on new test set\nprint(\"\\nEvaluating on New Test Set...\")\ntest_dir = \"/kaggle/input/mapping-isic2019-testing-dataset/ISIC_2019_Test_By_Class\"\ntest_dataset = datasets.ImageFolder(root=test_dir, transform=transform)  # Apply transform\ntest_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4, worker_init_fn=seed_worker, generator=g)\n\nbest_model = CNN_VGG8().to(device)\nbest_model.load_state_dict(torch.load(\"/kaggle/working/best_cancer_prediction.pth\", weights_only=True))\nif torch.cuda.device_count() > 1:\n    best_model = nn.DataParallel(best_model)  # Wrap for evaluation if multiple GPUs\nbest_model.eval()\n\ncorrect = 0\ntotal = 0\ntest_label_list = []\ntest_predict_list = []\ntest_probs_list = []\n\nwith torch.no_grad():\n    for i, data in enumerate(test_dataloader):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        out = best_model(inputs)\n        probs = F.softmax(out, dim=1)\n        _, predicted = torch.max(out, 1)\n        test_label_list.extend(labels.cpu().numpy())\n        test_predict_list.extend(predicted.cpu().numpy())\n        test_probs_list.extend(probs.cpu().numpy())\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n# Compute test set metrics\ntest_accuracy = 100 * correct / total\ntest_report = classification_report(test_label_list, test_predict_list, target_names=class_names, output_dict=True, zero_division=0)\ntest_conf_matrix = confusion_matrix(test_label_list, test_predict_list)\n\n# Save final test confusion matrix\nplot_confusion_matrix(\n    test_conf_matrix, class_names,\n    title='Final Confusion Matrix (Test Set)',\n    filename='final_confusion_matrix_test.png'\n)\n\n# ROC Curve for test set\ntest_fpr = {}\ntest_tpr = {}\ntest_roc_auc = {}\nfor i in range(len(class_names)):\n    test_fpr[i], test_tpr[i], _ = roc_curve(np.array(test_label_list) == i, np.array(test_probs_list)[:, i])\n    test_roc_auc[i] = auc(test_fpr[i], test_tpr[i])\n\n# Print test set metrics\nprint(f\"\\nTest Set Metrics:\")\nprint(f\"Accuracy: {test_accuracy:.2f}%\")\nprint(f\"Precision: {test_report['weighted avg']['precision'] * 100:.2f}%\")\nprint(f\"Recall: {test_report['weighted avg']['recall'] * 100:.2f}%\")\nprint(f\"F1 Score: {test_report['weighted avg']['f1-score'] * 100:.2f}%\")\nprint(\"Confusion Matrix:\\n\", test_conf_matrix)\nprint(\"Classification Report:\\n\", classification_report(test_label_list, test_predict_list, target_names=class_names, zero_division=0))\n\n# Plot ROC Curve for test set\nplt.figure(figsize=(10, 8))\nfor i in range(len(class_names)):\n    plt.plot(test_fpr[i], test_tpr[i], label=f'{class_names[i]} (AUC = {test_roc_auc[i]:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Test Set')\nplt.legend(loc=\"lower right\")\nplt.savefig('roc_curve_test_set.png')\nplt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}